---
title: AGI and 
description: | 
  
author:

output: 
  radix::radix_article:
    toc: true
    toc_depth: 1
---
# Introduction
## Intro 

Whether it's been science, philosophy, or artificial intelligence, huamnswe have always found ourselves at odds with our existences. Why do we exist? Many have tired to approach this question with varying degrees of illumination. It But there are still more questions than there are answers. Artificial intelligence has become a salient mirror just as the industrial revolution made us realise what type of work "humans" should be doing, and therefore illuminated what we valued as humans. F We moved away from the endlessness of the question of why are we here and immediately moved to what Bourdieu called Distinction. What makes us different?. Of all the sciences that have begin to answer the question, progress in the field of neurobiology, computer science, artificial intelligence, primatology, anthropology, and sociology have all played a role in thinning the line between us and other animals just as Netwonian physics have told us that we may not be alone, that we may not be the centre of the universe. Monkeys with tools, dolphins and casual sex, Elephans and their endless memory, and the bees way of social organising shortened our view of humans as distinct.ButAs Ernest Becker has said, we are half animal and half symbolic andso we find ourselves valuing what isseems more abstract and less grounded in the flesh. Descarte and Aristole came to similar conclusion sabout the question. What makes us hman is reason, our ability to detached ourselves from our feelings and reason. But how to reason properly? . We will In parallelle other scientists looked to Darwin and a more biological view of intelligence. As thestudies on the nervous system became more and more advanceds, we moved from psychological theory grounded in observational research to looking  at networks of neurons and how they react to certain stimulus. This lead to neuroscience which was slowly uncovering that our central nervous system, i tthe key to our most profound features. Our cognition, thinking, reasoning, perceiving, and learning and memory. This would eventually lead to the cognitive revolution where many scientists from differing fields came to study the brain  and mental activitiy happeningin ourselves and other animals. It is obvious now, that this would lead to a want to emulate the central nervous system and . Can we emulate the brain and would that then give us keys to intelligence? Could that give us have conciousness an? AI seemed is an approach to answer such questions. We still no very little about how we reason an, learn, and experience everyday life. We can see AI as two tracks, applied AI, and philosophical AI. The first deals with cerating industrial applications and the second AIis around understanding intelligence. AI is both a tool and a mirror. So the hype will always exist because we do not know what makes us human and what we can and cannot emulate. AI has a chance at being a tool we use to make the world better or worse, and it also gives us a mirror into our world and what it means to exist. 

## Logic & Reasoning

And so we begin with reasoning. It didn't take very long for humans to begin to propose answers to the question of what does it mean exist. Dating back to 300 BC, their were major figures across many the field of philosophy, sociology, economics, and mathematics have looked to reasoning as one of the key distinctions of humankind. 

Our ability to take a set of propositions and to know their truth and false values. Only from this base of propositions can we begin to conclud eor make logical combinations to create valid knowledge.

Aristotle, called humans "animal rationale", in his treatise, On the Soul,  who saw humans as distinguised by their rationality, proposed the syllogism; Which was one of the first formulations of logic around 300 B.C. found in the Organon. In 1275, Ramon Llull, a spanish Theologian wrote the Ars Magna (Art of Finding Truth), which provided a method based in logic to produce new knowledge. This would eventually be further by  Giordano Bruno and Gottfried Leibiniz in his dissertation titled 'ars combinatoria'. It provided the possibility of creating a logic of invention. Leibiniz's focus influenceds things we know today as automated theorem prover. 


Many scientists tried to forward this idea of rationality and mechanise it.
Leibiniz would go on to create the logical calculii and then improved the  which was a mechanical calculator.


 Porphyry considered man to be animal + morality + and rationality.
 Weber consid
 
 Charles Babbage
 Ada Lovelace
 
 George Boole took the basics of logic and mechanised it into an algebra.
 This algebra would be combined with Claude Shannon's work to bring in the information age. 
 
 
 
## Reasoning Under Uncertainty
 
Although logic gave us a way to reason under specific conditions, those conditions didn't match the typical type of problem a human comes into contact with in the real world. As we orient ourselves towards concrete and abstract goals we often have to settle with an incomplete picture, lacking the certainty of propositions that logic requires. So we needed a way to reason under uncertainty, this brought on the theory of probabilities. 
 
Probability dates back to 1550 by Cardan, but wouldn't be of practical use until the correspondence of Ferment and Pascal in 1654. Pascal and Fermet were discussing the problem that Chevalier de Méré proposed to Pascal, titled the problem of points. 

The game went as follows:

Suppose two players who have equal chance of wining each round. Their is a prize pot where they have both contributed 50% each. The game has a priori number of rounds and who every wins the most rounds takes the prize pot. But let us suppose that the game is interupptd before the end of the game. How does one divide the pot fairly?

There were others how attempted to solve this problem but the current outcomes lead to edge cases that were unintuitive and were mostly based on the past games.

Pascal and Ferment made a breakthrough by considering (from the interruption forward) the possible futures that might be possible if the players had continued until the end. 

$$ r = rounds\ for\ p1\ to\ win $$
$$ s = rounds\ for\ p2\ to\ win $$
 $$ r + s - 1 $$
 
 $$ Number\ of\ possible\ outcomes\ 2^{r+s-1} $$ 
 Odds vs. expectations
 Soon after Pascale improving on Ferment's results, Christiaan Huygens, used Pascal's result to create the first treatise of modern Probability based on expectation.s
 
> For example, the probability of throwing a 6 on a die twice is 1⁄6 x 1⁄6 = 1⁄36 ("and" works like multiplication); the probability of throwing either a 3 or a 6 is 1⁄6 + 1⁄6 = 1⁄3 ("or" works like addition).
 
 They also figured out the problem 
 
 This formalisation would lead to the foundation of modern probability. 
 
 Pascal would go on to use probability in his most notable work, Pensees, where he made a convincing argument of why one should believe in God. 
 
 Nearly a centruy later, following the work of Fermet and Pascal, Abraham de Abraham de Moivre gave the first statement of the formula for the normal distribution which would be 
 
 Laplace would be even more practical than Fermet and Pascal in his book, Théorie Analytique des Probabilités, where he elucidates the use of probablity theory to scientific and practical problems in statistical mechanics, actuarial mathematics, error, and so on.
 
 Probability became a general workhorse. Andrey Andreyevich Markov introduced the Markov chains in 1906 when he produced the first theoretical results for stochastic processes by using the term “chain” for the first time. In 1913 he calculated letter sequences of the Russian language. 
 
 A generalization to countable infinite state spaces was given by Kolmogorov (1931). Markov chains are related to Brownian motion and the ergodic hypothesis, two topics in physics which were important in the early years of the twentieth century. But Markov appears to have pursued this out of a mathematical motivation, namely the extension of the law of large numbers to dependent events. Out of this approach grew a general stw a general statistical instrument, the so-called stochastic Markov process. 
 
 Not without st
  R. A. Fisher strongly used to recommend,
analyze and make the Maximum-likelihood popular between 1912 and 1922, although it had
been used earlier by Gauss, Laplace, Thiele, and F. Y. Edgeworth. Several years later the EM algorithm was explained and given its name in a paper 1977 by Arthur Dempster, Nan
Laird, and Donald Rubin in the Journal of the Royal Statistical Society. They pointed out
that the method had been "proposed many times in special circumstances" by other authors,
but the 1977 paper generalized the method and developed the theory behind it. An
expectation-maximization (EM) algorithm is used in statistics for finding maximum
likelihood estimates of parameters in probabilistic models, where the model depends on
unobserved latent variables. EM alternates between performing an expectation (E) step,
which computes an expectation of the likelihood by including the latent variables as if they 
 
 # Computer Sciences
 
 
 
 
 - Classical probability
 - Empirical probability
 - Subjective probability
 
 